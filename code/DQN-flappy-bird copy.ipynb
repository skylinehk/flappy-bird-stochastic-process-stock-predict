{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x28462739c40>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PYTORCH IMPORTS\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# OTHER USEFUL PYTHON MODULES AND PACKAGES\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import imageio\n",
    "import flappy_bird_gym\n",
    "\n",
    "# BASE PYTHON IMPORTS\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "from itertools import count\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# SET UP MATPLOTLIB\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell if matplotlib causes python kernel dead\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check your environment\n",
    "# import sys\n",
    "# print(sys.executable)\n",
    "# print(torch.__file__) \n",
    "# print(torch.cuda.is_available())\n",
    "# from torch.utils import collect_env\n",
    "# print(collect_env.main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install and import the flappy-bird-gym env\n",
    "# pip install flappy-bird-gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Observation Features: 1 \n",
      "\n",
      "# Possible Actions: 2 \n",
      "\n",
      "Initial Observation: \n",
      "\n",
      "[1.65625] \n",
      "\n",
      "Observation array is of type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "env = flappy_bird_gym.make(\"FlappyBird-v0\")\n",
    "state, info = env.reset()\n",
    "\n",
    "# Convert state to 1D numpy array\n",
    "state = np.array(state).flatten()\n",
    "\n",
    "# Print the number of observation features\n",
    "print('# Observation Features:', len(state), '\\n')\n",
    "\n",
    "# Print the number of possible actions\n",
    "print('# Possible Actions:', env.action_space.n, '\\n')\n",
    "\n",
    "# Print the initial observation\n",
    "print('Initial Observation:', '\\n')\n",
    "print(state, '\\n')\n",
    "\n",
    "# Print the type of the observation array\n",
    "print('Observation array is of type:', type(state))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model class DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    # DEFINE THE SPECIAL INIT METHOD\n",
    "    def __init__(self, features, hidden, output):\n",
    "        super(DQN, self).__init__()\n",
    "        self.linear1 = nn.Linear(features, hidden[0])\n",
    "        self.linear2 = nn.Linear(hidden[0], hidden[1])\n",
    "        self.linear3 = nn.Linear(hidden[1], output)\n",
    "        \n",
    "    # DEFINE THE FORWARD METHOD\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.linear1(x))\n",
    "        out = F.relu(self.linear2(out))\n",
    "        y_pred = self.linear3(out)\n",
    "        return y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\n",
    "    'Transition',\n",
    "    ('state', 'action', 'next_state', 'reward')\n",
    "    )\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \"\"\"\n",
    "    A class to store gym transitions\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function select action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_decay(steps_done):\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * np.exp(-1. * steps_done / EPS_DECAY)\n",
    "    return eps_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, eps):\n",
    "\n",
    "    num_actions = 2  \n",
    "\n",
    "    with torch.no_grad():\n",
    "        q_values = policy_net(state)\n",
    "        if random.random() > eps:\n",
    "            # Take the action with the highest Q-value\n",
    "            action = q_values.argmax().unsqueeze(0)\n",
    "        else:\n",
    "            # Take a random action\n",
    "            action = torch.tensor([random.randrange(num_actions)], device=device)\n",
    "    return action"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to optimize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "\n",
    "    # CHECK THE LENGTH OF THE REPLAY MEMORY, RETURN NONE IF LENGTH LESS THAN\n",
    "    # THE BATCH_SIZE\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return 0\n",
    "    \n",
    "    # SAMPLE N TRANSITIONS FROM THE REPLAY MEMORY, WHERE N IS BATCH_SIZE\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "\n",
    "    # DEFINE THE BATCH AS TRANSITION NAMED TUPLE, THIS SPLITS state's, \n",
    "    # action's, next_state's, and reward's into tuples see the following link\n",
    "    # https://stackoverflow.com/a/19343/3343043\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # FIND THE INDICES OF TRANSITIONS THAT ARE NON-TERMINAL STATES\n",
    "    non_final_mask = torch.tensor(\n",
    "        tuple(map(lambda s: s is not None, batch.next_state)), \n",
    "        device=device, \n",
    "        dtype=torch.bool\n",
    "        )\n",
    "    \n",
    "    # FIND THAT STATES THAT ARE NON-TERMINAL STATES\n",
    "    non_final_next_states = torch.cat(\n",
    "        [s for s in batch.next_state if s is not None]\n",
    "        )\n",
    "    \n",
    "\n",
    "    ### CREATE 3 TENSORS ONE FOR THE STATES, ACTIONS AND REWARDS\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action).unsqueeze(1)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "\n",
    "    # GET THE ACTION VALUES OF POLICY NET FOR EACH ACTION WHICH WAS TAKEN\n",
    "    # HERE YOU SHOULD USE tensor.gather() METHOD SO YOU CAN SPECIFY WHICH \n",
    "    # ACTION WE ARE INTERESTED IN. WE ONLY WANT THE ACTION WHICH WAS TAKEN\n",
    "    # FOR THE TRANSACTION\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # CREATE TENSOR OF ZEROS TO STORE THE NEXT STATE VALUES WITH torch.zeros()\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    \n",
    "    # DO NOT CALCULATE GRADIENTS\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # DEFINE THE NEXT STATE Q VALUES USING THE TARGET NET - max(Q_target(s', a'))\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "        \n",
    "    # COMPUTE EXPECTED RETURN Y = Discount*max(Q_target(s',a)) + Reward \n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # DEFINE THE HUBER LOSS\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "\n",
    "    # CALCULATE THE LOSS L(Q_policy(s,a), Y)\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # ZERO THE GRADIENTS\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # PROPAGATE GRADIENT THROUGH NETWORK \n",
    "    loss.backward()\n",
    "\n",
    "    # IN PLACE GRADIENT CLIPPING\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "\n",
    "    # TAKE AN OPTIMIZER STEP\n",
    "    optimizer.step()\n",
    "\n",
    "    return 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions to show training progess\n",
    "The plot shows moving average of rewards for the 10 most recent episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = []\n",
    "episode_rewards = []\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # # Take 100 episode averages and plot them too\n",
    "    # if len(durations_t) >= 100:\n",
    "    #     means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "    #     means = torch.cat((torch.zeros(99), means))\n",
    "    #     plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "\n",
    "def plot_rewards(show_result=False):\n",
    "    # SET THE CURRENT FIGURE\n",
    "    plt.figure(1)\n",
    "\n",
    "    # IF TRAINING IS COMPLETE\n",
    "    if show_result:\n",
    "        plt.title(f'MA10 Result (Episode {i_episode + 1}Highest Reward: {max(episode_rewards)}')\n",
    "    \n",
    "    # IF STILL TRAINING   \n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title(f'Training... Eps: {i_episode + 1} Current Reward: {cum_reward} Highest Reward: {max(episode_rewards)}')\n",
    "    \n",
    "    # SET LABELS AND PLOT DATA\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.plot(episode_rewards)\n",
    "    # PAUSE SO THAT PLOTS CAN BE UPDATED\n",
    "    plt.pause(0.001)\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            # WAIT UNTIL NEW OUTPUT AVAILABLE TO CLEAR CURRENT FIGURE\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model and training parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment\n",
    "env = flappy_bird_gym.make(\"FlappyBird-v0\")\n",
    "\n",
    "# Set the device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define all the scalar constants\n",
    "BATCH_SIZE = 258\n",
    "GAMMA = 0.99\n",
    "EPS_START = 1\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 10000\n",
    "TAU = 0.001\n",
    "LR = 0.001\n",
    "HIDDEN = 258\n",
    "MEMORY = 100000\n",
    "\n",
    "# Get the number of actions from the environment\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Reset the environment\n",
    "state = env.reset()\n",
    "\n",
    "# Get the number of features in a state\n",
    "n_features = state.shape[0]\n",
    "\n",
    "# Define the policy and target net as instances of the DQN model\n",
    "policy_net = DQN(n_features, [HIDDEN, HIDDEN], n_actions).to(device)\n",
    "target_net = DQN(n_features, [HIDDEN, HIDDEN], n_actions).to(device)\n",
    "\n",
    "# Sync the weights of the target and policy nets\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# Define the optimizer - use AdamW with amsgrad=True\n",
    "optimizer = torch.optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "\n",
    "# Define the replay memory as an instance of ReplayMemory\n",
    "replay_buffer = ReplayMemory(MEMORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The observation:\n",
      "\n",
      "tensor([[1., 2.]], device='cuda:0') \n",
      "\n",
      "The policy net predictions:\n",
      "\n",
      "tensor([[-0.0716, -0.2946]], device='cuda:0') \n",
      "\n",
      "Action: 0 \n",
      "\n",
      "Using the select action function: \n",
      "\n",
      "Action: tensor([0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# TEST THE MODEL\n",
    "ini_state = [1.0, 2.0]\n",
    "state = ini_state\n",
    "state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "# Define the policy and target net as instances of the updated DQN model\n",
    "policy_net = DQN(n_features, [HIDDEN, HIDDEN], n_actions).to(device)\n",
    "target_net = DQN(n_features, [HIDDEN, HIDDEN], n_actions).to(device)\n",
    "\n",
    "# Sync the weights of the target and policy nets\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('The observation:\\n')\n",
    "print(state_tensor.to(device), '\\n')\n",
    "\n",
    "print('The policy net predictions:\\n')\n",
    "with torch.no_grad():\n",
    "    q_values = policy_net(state_tensor.to(device))\n",
    "    print(q_values, '\\n')\n",
    "    action = q_values.argmax().item()\n",
    "    print('Action:', action, '\\n')\n",
    "\n",
    "print('Using the select action function:', '\\n')\n",
    "action = select_action(state_tensor.to(device), eps=0)\n",
    "print('Action:', action)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CHECK IF CUDA IS AVAILABLE\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# DATA STRUCTURES TO STORE RELEVANT DATA FOR FUTURE PLOTS\n",
    "episode_durations = []\n",
    "episode_rewards = []\n",
    "total_transitions = 0\n",
    "start_time = time.time()\n",
    "num_episodes = 1000\n",
    "memory = ReplayMemory(MEMORY)\n",
    "\n",
    "# PERFORM SET NUMBER OF EPISODES\n",
    "for i_episode in range(num_episodes):\n",
    "\n",
    "    # START THE ENVIRONMENT AND OBSERVE THE STATE\n",
    "    state = env.reset()\n",
    "\n",
    "    # CONVERT THE 8 ELEMENT ARRAY TO TORCH TENSOR AND MOVE TO DEVICE\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    cum_reward = 0\n",
    "    total_transitions = 0\n",
    "    \n",
    "    # FOR EVERY TRANSITION IN THE EPISODE (ITERATES UNTIL TERMINAL STATE)\n",
    "    while True:\n",
    "        # print(f\"Step: {total_transitions}, Reward: {cum_reward}\")\n",
    "        # UPDATE TOTAL TRANSITIONS COUNTER\n",
    "        total_transitions += 1\n",
    "\n",
    "        # CALCULATE EPS USING TOTAL TRANSITIONS\n",
    "        eps = eps_decay(total_transitions)\n",
    "\n",
    "        # SELECT AN ACTION USING THE TARGET NETWORK - select_action() FUNCTION\n",
    "        action = select_action(state, eps=eps)\n",
    "\n",
    "        # OBSERVE THE TRANSITION\n",
    "        observation, reward, done, _ = env.step(action.item())\n",
    "        cum_reward += reward\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        if reward < 0:\n",
    "            done = True\n",
    "            \n",
    "        # DEFINE THE NEXT STATE OF THE ENVIRONMENT\n",
    "        if done:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # PUSH THE TRANSITION INTO THE REPLAY MEMORY\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # STORE THE NEXT STATE AS CURRENT STATE FOR NEXT ITERATION\n",
    "        state = next_state\n",
    "\n",
    "        # OPTIMIZE THE MODEL (UPDATE THE WEIGHTS OF THE POLICY NET)\n",
    "        optimize_model()\n",
    "\n",
    "        # GET THE CURRENT WEIGHTS OF EACH NETWORK\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "\n",
    "        # CALCULATE SOFT UPDATE OF TARGET NET WEIGHTS\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        for key in policy_net_state_dict:\n",
    "            # MOVE THE POLICY_NET_STATE_DICT TENSOR TO THE CPU DEVICE\n",
    "            policy_net_state_dict[key] = policy_net_state_dict[key].to(device)\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "            target_net_state_dict[key] = target_net_state_dict[key].to(device)\n",
    "\n",
    "        # APPLY THE UPDATE TO THE TARGET NETWORK\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        # IF THE EPISODE IS COMPLETE, STORE THE EPISODE DURATION AND REWARD AND BREAK LOOP\n",
    "        if done:\n",
    "            episode_durations.append(total_transitions)\n",
    "            episode_rewards.append(cum_reward)\n",
    "            # plot_durations()\n",
    "            plot_rewards()\n",
    "            break\n",
    "\n",
    "        # STOP THE EPISODE WHEN REWARDS ARE LESS THAN ZERO\n",
    "        if cum_reward < 0:\n",
    "            done = True\n",
    "            episode_durations.append(total_transitions)\n",
    "            episode_rewards.append(cum_reward)\n",
    "            plot_rewards()\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "end_time = time.time()\n",
    "plot_rewards(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7656.7445821762085\n"
     ]
    }
   ],
   "source": [
    "print(end_time - start_time)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Check The Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('./models/flappy_bird_dqn_linear_1.pth')\n",
    "torch.save(policy_net, './models/flappy_bird_dqn_linear_1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Cumulative Reward = 101\n",
      "Episode 2: Cumulative Reward = 101\n",
      "Episode 3: Cumulative Reward = 107\n",
      "Episode 4: Cumulative Reward = 101\n",
      "Episode 5: Cumulative Reward = 101\n",
      "Episode 6: Cumulative Reward = 105\n",
      "Episode 7: Cumulative Reward = 101\n",
      "Episode 8: Cumulative Reward = 103\n",
      "Episode 9: Cumulative Reward = 101\n",
      "Episode 10: Cumulative Reward = 101\n",
      "Episode 11: Cumulative Reward = 103\n",
      "Episode 12: Cumulative Reward = 103\n",
      "Episode 13: Cumulative Reward = 101\n",
      "Episode 14: Cumulative Reward = 103\n",
      "Episode 15: Cumulative Reward = 101\n",
      "Episode 16: Cumulative Reward = 101\n",
      "Episode 17: Cumulative Reward = 101\n",
      "Episode 18: Cumulative Reward = 101\n",
      "Episode 19: Cumulative Reward = 101\n",
      "Episode 20: Cumulative Reward = 103\n",
      "Average Reward over 20 Episodes = 102.00\n"
     ]
    }
   ],
   "source": [
    "# Set up the Flappy Bird environment\n",
    "env = flappy_bird_gym.make(\"FlappyBird-v0\")\n",
    "\n",
    "# Set up the DQN policy network\n",
    "input_dim = 2  # Replace with the actual input dimensions for your environment\n",
    "output_dim = env.action_space.n\n",
    "hidden_dim = 64\n",
    "\n",
    "# Play the game using the trained policy network for 20 episodes\n",
    "num_episodes = 20\n",
    "episode_rewards = []\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    cum_reward = 0\n",
    "\n",
    "    while True:\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        q_values = policy_net(state)\n",
    "        action = q_values.argmax(dim=1).item()\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        cum_reward += reward\n",
    "\n",
    "        if done:\n",
    "            episode_rewards.append(cum_reward)\n",
    "            print(\"Episode {}: Cumulative Reward = {}\".format(i_episode+1, cum_reward))\n",
    "            break\n",
    "\n",
    "# Print the average rewards over 20 episodes\n",
    "avg_reward = sum(episode_rewards) / len(episode_rewards)\n",
    "print(\"Average Reward over 20 Episodes = {:.2f}\".format(avg_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "#policy_net = torch.load('./models/flappy_bird_dqn_linear_1.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve the DQN network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class enhancedDQN(nn.Module):\n",
    "    def __init__(self, n_features, hidden, n_actions):\n",
    "        super(enhancedDQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_features, hidden[0])\n",
    "        self.fc2 = nn.Linear(hidden[0], hidden[1])\n",
    "        self.fc3 = nn.Linear(hidden[1], n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "                nn.init.constant_(m.bias, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "\n",
    "    # CHECK THE LENGTH OF THE REPLAY MEMORY, RETURN NONE IF LENGTH LESS THAN\n",
    "    # THE BATCH_SIZE\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return 0\n",
    "    \n",
    "    # SAMPLE N TRANSITIONS FROM THE REPLAY MEMORY, WHERE N IS BATCH_SIZE\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "\n",
    "    # DEFINE THE BATCH AS TRANSITION NAMED TUPLE, THIS SPLITS state's, \n",
    "    # action's, next_state's, and reward's into tuples see the following link\n",
    "    # https://stackoverflow.com/a/19343/3343043\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # FIND THE INDICES OF TRANSITIONS THAT ARE NON-TERMINAL STATES\n",
    "    non_final_mask = torch.tensor(\n",
    "        tuple(map(lambda s: s is not None, batch.next_state)), \n",
    "        device=device, \n",
    "        dtype=torch.bool\n",
    "        )\n",
    "    \n",
    "    # FIND THAT STATES THAT ARE NON-TERMINAL STATES\n",
    "    non_final_next_states = torch.cat(\n",
    "        [s for s in batch.next_state if s is not None]\n",
    "        )\n",
    "    \n",
    "\n",
    "    ### CREATE 3 TENSORS ONE FOR THE STATES, ACTIONS AND REWARDS\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action).unsqueeze(1)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "\n",
    "    # GET THE ACTION VALUES OF POLICY NET FOR EACH ACTION WHICH WAS TAKEN\n",
    "    # HERE YOU SHOULD USE tensor.gather() METHOD SO YOU CAN SPECIFY WHICH \n",
    "    # ACTION WE ARE INTERESTED IN. WE ONLY WANT THE ACTION WHICH WAS TAKEN\n",
    "    # FOR THE TRANSACTION\n",
    "    state_action_values = policy_net(state_batch)\n",
    "\n",
    "    # CREATE TENSOR OF ZEROS TO STORE THE NEXT STATE VALUES WITH torch.zeros()\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    \n",
    "    # DO NOT CALCULATE GRADIENTS\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # DEFINE THE NEXT STATE Q VALUES USING THE TARGET NET - max(Q_target(s', a'))\n",
    "        next_state_actions = policy_net(non_final_next_states)\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "\n",
    "    # COMPUTE EXPECTED RETURN Y = Discount*max(Q_target(s',a)) + Reward \n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # DEFINE THE HUBER LOSS\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "\n",
    "    # CALCULATE THE LOSS L(Q_policy(s,a), Y)\n",
    "    loss = criterion(state_action_values.gather(1, action_batch), expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # ZERO THE GRADIENTS\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # PROPAGATE GRADIENT THROUGH NETWORK \n",
    "    loss.backward()\n",
    "\n",
    "    # IN PLACE GRADIENT CLIPPING\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "\n",
    "    # TAKE AN OPTIMIZER STEP\n",
    "    optimizer.step()\n",
    "\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment\n",
    "env = flappy_bird_gym.make(\"FlappyBird-v0\")\n",
    "\n",
    "# Set the device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define all the scalar constants\n",
    "BATCH_SIZE = 258\n",
    "GAMMA = 0.99\n",
    "EPS_START = 1\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 10000\n",
    "TAU = 0.001\n",
    "LR = 0.001\n",
    "HIDDEN = 258\n",
    "MEMORY = 1000000\n",
    "\n",
    "# Get the number of actions from the environment\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Reset the environment\n",
    "state = env.reset()\n",
    "\n",
    "# Get the number of features in a state\n",
    "n_features = state.shape[0]\n",
    "\n",
    "# Define the policy and target net as instances of the updated DQN model\n",
    "policy_net = enhancedDQN(n_features, [HIDDEN, HIDDEN], n_actions).to(device)\n",
    "target_net = enhancedDQN(n_features, [HIDDEN, HIDDEN], n_actions).to(device)\n",
    "\n",
    "# Sync the weights of the target and policy nets\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# Define the optimizer - use AdamW with amsgrad=True\n",
    "optimizer = torch.optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "\n",
    "# Define the replay memory as an instance of ReplayMemory\n",
    "replay_buffer = ReplayMemory(MEMORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The observation:\n",
      "\n",
      "tensor([[1., 2.]], device='cuda:0') \n",
      "\n",
      "The policy net predictions:\n",
      "\n",
      "tensor([[0.1068, 0.0066]], device='cuda:0') \n",
      "\n",
      "Action: 0 \n",
      "\n",
      "Using the select action function: \n",
      "\n",
      "Action: tensor([0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# TEST THE MODEL\n",
    "ini_state = [1.0, 2.0]\n",
    "state = ini_state\n",
    "state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "# Define the policy and target net as instances of the updated DQN model\n",
    "policy_net = enhancedDQN(n_features, [HIDDEN, HIDDEN], n_actions).to(device)\n",
    "target_net = enhancedDQN(n_features, [HIDDEN, HIDDEN], n_actions).to(device)\n",
    "\n",
    "# Sync the weights of the target and policy nets\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('The observation:\\n')\n",
    "print(state_tensor.to(device), '\\n')\n",
    "\n",
    "print('The policy net predictions:\\n')\n",
    "with torch.no_grad():\n",
    "    q_values = policy_net(state_tensor.to(device))\n",
    "    print(q_values, '\\n')\n",
    "    action = q_values.argmax().item()\n",
    "    print('Action:', action, '\\n')\n",
    "\n",
    "print('Using the select action function:', '\\n')\n",
    "action = select_action(state_tensor.to(device), eps=0)\n",
    "print('Action:', action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.94791667, 0.51757812]), 1, False, {'score': 0})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(action.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Reward: tensor([1], device='cuda:0')\n",
      "Step: 1, Reward: tensor([1], device='cuda:0')\n",
      "Step: 2, Reward: tensor([1], device='cuda:0')\n",
      "Step: 3, Reward: tensor([1], device='cuda:0')\n",
      "Step: 4, Reward: tensor([1], device='cuda:0')\n",
      "Step: 5, Reward: tensor([1], device='cuda:0')\n",
      "Step: 6, Reward: tensor([1], device='cuda:0')\n",
      "Step: 7, Reward: tensor([1], device='cuda:0')\n",
      "Step: 8, Reward: tensor([1], device='cuda:0')\n",
      "Step: 9, Reward: tensor([1], device='cuda:0')\n",
      "Step: 10, Reward: tensor([1], device='cuda:0')\n",
      "Step: 11, Reward: tensor([1], device='cuda:0')\n",
      "Step: 12, Reward: tensor([1], device='cuda:0')\n",
      "Step: 13, Reward: tensor([1], device='cuda:0')\n",
      "Step: 14, Reward: tensor([1], device='cuda:0')\n",
      "Step: 15, Reward: tensor([1], device='cuda:0')\n",
      "Step: 16, Reward: tensor([1], device='cuda:0')\n",
      "Step: 17, Reward: tensor([1], device='cuda:0')\n",
      "Step: 18, Reward: tensor([1], device='cuda:0')\n",
      "Step: 19, Reward: tensor([1], device='cuda:0')\n",
      "Step: 20, Reward: tensor([1], device='cuda:0')\n",
      "Step: 21, Reward: tensor([1], device='cuda:0')\n",
      "Step: 22, Reward: tensor([1], device='cuda:0')\n",
      "Step: 23, Reward: tensor([1], device='cuda:0')\n",
      "Step: 24, Reward: tensor([1], device='cuda:0')\n",
      "Step: 25, Reward: tensor([1], device='cuda:0')\n",
      "Step: 26, Reward: tensor([1], device='cuda:0')\n",
      "Step: 27, Reward: tensor([1], device='cuda:0')\n",
      "Step: 28, Reward: tensor([1], device='cuda:0')\n",
      "Step: 29, Reward: tensor([1], device='cuda:0')\n",
      "Step: 30, Reward: tensor([1], device='cuda:0')\n",
      "Step: 31, Reward: tensor([1], device='cuda:0')\n",
      "Step: 32, Reward: tensor([1], device='cuda:0')\n",
      "Step: 33, Reward: tensor([1], device='cuda:0')\n",
      "Step: 34, Reward: tensor([1], device='cuda:0')\n",
      "Step: 35, Reward: tensor([1], device='cuda:0')\n",
      "Step: 36, Reward: tensor([1], device='cuda:0')\n",
      "Step: 37, Reward: tensor([1], device='cuda:0')\n",
      "Step: 38, Reward: tensor([1], device='cuda:0')\n",
      "Step: 39, Reward: tensor([1], device='cuda:0')\n",
      "Step: 40, Reward: tensor([1], device='cuda:0')\n",
      "Step: 41, Reward: tensor([1], device='cuda:0')\n",
      "Step: 42, Reward: tensor([1], device='cuda:0')\n",
      "Step: 43, Reward: tensor([1], device='cuda:0')\n",
      "Step: 44, Reward: tensor([1], device='cuda:0')\n",
      "Step: 45, Reward: tensor([1], device='cuda:0')\n",
      "Step: 46, Reward: tensor([1], device='cuda:0')\n",
      "Step: 47, Reward: tensor([1], device='cuda:0')\n",
      "Step: 48, Reward: tensor([1], device='cuda:0')\n",
      "Step: 49, Reward: tensor([1], device='cuda:0')\n",
      "Step: 50, Reward: tensor([1], device='cuda:0')\n",
      "Step: 51, Reward: tensor([1], device='cuda:0')\n",
      "Step: 52, Reward: tensor([1], device='cuda:0')\n",
      "Step: 53, Reward: tensor([1], device='cuda:0')\n",
      "Step: 54, Reward: tensor([1], device='cuda:0')\n",
      "Step: 55, Reward: tensor([1], device='cuda:0')\n",
      "Step: 56, Reward: tensor([1], device='cuda:0')\n",
      "Step: 57, Reward: tensor([1], device='cuda:0')\n",
      "Step: 58, Reward: tensor([1], device='cuda:0')\n",
      "Step: 59, Reward: tensor([1], device='cuda:0')\n",
      "Step: 60, Reward: tensor([1], device='cuda:0')\n",
      "Step: 61, Reward: tensor([1], device='cuda:0')\n",
      "Step: 62, Reward: tensor([1], device='cuda:0')\n",
      "Step: 63, Reward: tensor([1], device='cuda:0')\n",
      "Step: 64, Reward: tensor([1], device='cuda:0')\n",
      "Step: 65, Reward: tensor([1], device='cuda:0')\n",
      "Step: 66, Reward: tensor([1], device='cuda:0')\n",
      "Step: 67, Reward: tensor([1], device='cuda:0')\n",
      "Step: 68, Reward: tensor([1], device='cuda:0')\n",
      "Step: 69, Reward: tensor([1], device='cuda:0')\n",
      "Step: 70, Reward: tensor([1], device='cuda:0')\n",
      "Step: 71, Reward: tensor([1], device='cuda:0')\n",
      "Step: 72, Reward: tensor([1], device='cuda:0')\n",
      "Step: 73, Reward: tensor([1], device='cuda:0')\n",
      "Step: 74, Reward: tensor([1], device='cuda:0')\n",
      "Step: 75, Reward: tensor([1], device='cuda:0')\n",
      "Step: 76, Reward: tensor([1], device='cuda:0')\n",
      "Step: 77, Reward: tensor([1], device='cuda:0')\n",
      "Step: 78, Reward: tensor([1], device='cuda:0')\n",
      "Step: 79, Reward: tensor([1], device='cuda:0')\n",
      "Step: 80, Reward: tensor([1], device='cuda:0')\n",
      "Step: 81, Reward: tensor([1], device='cuda:0')\n",
      "Step: 82, Reward: tensor([1], device='cuda:0')\n",
      "Step: 83, Reward: tensor([1], device='cuda:0')\n",
      "Step: 84, Reward: tensor([1], device='cuda:0')\n",
      "Step: 85, Reward: tensor([1], device='cuda:0')\n",
      "Step: 86, Reward: tensor([1], device='cuda:0')\n",
      "Step: 87, Reward: tensor([1], device='cuda:0')\n",
      "Step: 88, Reward: tensor([1], device='cuda:0')\n",
      "Step: 89, Reward: tensor([1], device='cuda:0')\n",
      "Step: 90, Reward: tensor([1], device='cuda:0')\n",
      "Step: 91, Reward: tensor([1], device='cuda:0')\n",
      "Step: 92, Reward: tensor([1], device='cuda:0')\n",
      "Step: 93, Reward: tensor([1], device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m observation, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action\u001b[39m.\u001b[39mitem())\n\u001b[0;32m     36\u001b[0m cum_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[1;32m---> 37\u001b[0m reward \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor([reward], device\u001b[39m=\u001b[39;49mdevice)\n\u001b[0;32m     39\u001b[0m \u001b[39m# STOP THE EPISODE WHEN REWARDS ARE LESS THAN ZERO\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[39mif\u001b[39;00m reward \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CHECK IF CUDA IS AVAILABLE\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# DATA STRUCTURES TO STORE RELEVANT DATA FOR FUTURE PLOTS\n",
    "episode_durations = []\n",
    "episode_rewards = []\n",
    "total_transitions = 0\n",
    "start_time = time.time()\n",
    "num_episodes = 10000\n",
    "memory = ReplayMemory(MEMORY)\n",
    "\n",
    "# PERFORM SET NUMBER OF EPISODES\n",
    "for i_episode in range(num_episodes):\n",
    "\n",
    "    # START THE ENVIRONMENT AND OBSERVE THE STATE\n",
    "    state = env.reset()\n",
    "\n",
    "    # CONVERT THE 8 ELEMENT ARRAY TO TORCH TENSOR AND MOVE TO DEVICE\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    cum_reward = 0\n",
    "    \n",
    "    # WHILE THE EPISODE IS NOT DONE\n",
    "    while True:\n",
    "        print(f\"Step: {total_transitions}, Reward: {reward}\")\n",
    "        # UPDATE TOTAL TRANSITIONS COUNTER\n",
    "        total_transitions += 1\n",
    "\n",
    "        # CALCULATE EPS USING TOTAL TRANSITIONS\n",
    "        eps = eps_decay(total_transitions)\n",
    "\n",
    "        # SELECT AN ACTION USING THE TARGET NETWORK - select_action() FUNCTION\n",
    "        action = select_action(state, eps=eps)\n",
    "\n",
    "        # OBSERVE THE TRANSITION\n",
    "        observation, reward, done, _ = env.step(action.item())\n",
    "        cum_reward += reward\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        # STOP THE EPISODE WHEN REWARDS ARE LESS THAN ZERO\n",
    "        if reward < 0:\n",
    "            done = True\n",
    "\n",
    "        # DEFINE THE NEXT STATE OF THE ENVIRONMENT\n",
    "        if done:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # PUSH THE TRANSITION INTO THE REPLAY MEMORY\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # STORE THE NEXT STATE AS CURRENT STATE FOR NEXT ITERATION\n",
    "        state = next_state\n",
    "\n",
    "        # OPTIMIZE THE MODEL (UPDATE THE WEIGHTS OF THE POLICY NET)\n",
    "        optimize_model()\n",
    "\n",
    "        # GET THE CURRENT WEIGHTS OF EACH NETWORK\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "\n",
    "        # CALCULATE SOFT UPDATE OF TARGET NET WEIGHTS\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        for key in policy_net_state_dict:\n",
    "            # MOVE THE POLICY_NET_STATE_DICT TENSOR TO THE CPU DEVICE\n",
    "            policy_net_state_dict[key] = policy_net_state_dict[key].to(device)\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "            target_net_state_dict[key] = target_net_state_dict[key].to(device)\n",
    "\n",
    "        # APPLY THE UPDATE TO THE TARGET NETWORK\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        # IF THE EPISODE IS COMPLETE, STORE THE EPISODE DURATION AND REWARD AND BREAK LOOP\n",
    "        if done:\n",
    "            episode_durations.append(total_transitions)\n",
    "            total_transitions = 0\n",
    "            episode_rewards.append(cum_reward)\n",
    "            cum_reward = 0\n",
    "            # plot_durations()\n",
    "            plot_rewards()\n",
    "            break\n",
    "\n",
    "        # STOP THE EPISODE WHEN REWARDS ARE LESS THAN ZERO\n",
    "        if cum_reward < 0:\n",
    "            done = True\n",
    "            episode_durations.append(total_transitions)\n",
    "            total_transitions = 0\n",
    "            episode_rewards.append(cum_reward)\n",
    "            cum_reward = 0\n",
    "            plot_rewards()\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "end_time = time.time()\n",
    "plot_rewards(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('./models/flappy_bird_newdqn.pth')\n",
    "torch.save(policy_net, './models/flappy_bird_newdqn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Flappy Bird environment\n",
    "env = flappy_bird_gym.make(\"FlappyBird-v0\")\n",
    "\n",
    "# Set up the DQN policy network\n",
    "input_dim = 2  # Replace with the actual input dimensions for your environment\n",
    "output_dim = env.action_space.n\n",
    "hidden_dim = 64\n",
    "\n",
    "# Play the game using the trained policy network for 20 episodes\n",
    "num_episodes = 20\n",
    "episode_rewards = []\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    cum_reward = 0\n",
    "\n",
    "    while True:\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        q_values = policy_net(state)\n",
    "        action = q_values.argmax(dim=1).item()\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        cum_reward += reward\n",
    "\n",
    "        if done:\n",
    "            episode_rewards.append(cum_reward)\n",
    "            print(\"Episode {}: Cumulative Reward = {}\".format(i_episode+1, cum_reward))\n",
    "            break\n",
    "\n",
    "# Print the average rewards over 20 episodes\n",
    "avg_reward = sum(episode_rewards) / len(episode_rewards)\n",
    "print(\"Average Reward over 20 Episodes = {:.2f}\".format(avg_reward))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
